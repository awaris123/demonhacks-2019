{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP Implementation for Flu Classifier\n",
    "\n",
    "Here is the improved classifier with Natural Language Processing (nltk library). \n",
    "- Parsing is done using built-in functions.\n",
    "- SnowballStemmer is picked for the stemming tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "provis maximum multipli owe care on go gone go wa thi\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "porterStemmer = PorterStemmer()\n",
    "\n",
    "sentence = \"Provision Maximum multiply owed caring on go gone going was this\"\n",
    "wordList = nltk.word_tokenize(sentence)\n",
    "\n",
    "stemWords = [porterStemmer.stem(word) for word in wordList]\n",
    "\n",
    "print(' '.join(stemWords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have a fever runny-nos and depress . not that my headach is that bad .\n"
     ]
    }
   ],
   "source": [
    "sentence = \"I have a fever runny-nose and depression. Not that my headache is that bad.\"\n",
    "wordList = nltk.word_tokenize(sentence)\n",
    "\n",
    "stemWords = [porterStemmer.stem(word) for word in wordList]\n",
    "\n",
    "print(' '.join(stemWords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "provis maximum multipli owe care on go gone go was this\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "snowBallStemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "sentence = \"Provision Maximum multiply owed caring on go gone going was this\"\n",
    "wordList = nltk.word_tokenize(sentence)\n",
    "\n",
    "stemWords = [snowBallStemmer.stem(word) for word in wordList]\n",
    "\n",
    "print(' '.join(stemWords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i have a fever runny-nos and depress . not that my headach is that bad .\n"
     ]
    }
   ],
   "source": [
    "sentence = \"I have a fever runny-nose and depression. Not that my headache is that bad.\"\n",
    "wordList = nltk.word_tokenize(sentence)\n",
    "stemWords = [snowBallStemmer.stem(word) for word in wordList]\n",
    "\n",
    "print(' '.join(stemWords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Conclusion:* Both of the stemming tools are working fine. We'll just stick with SnowBall since it was initially advised as a better tool. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pain blood fever hospit surgeri cramp accid injuri shot vaccin stress flu bleed headach nasal bloat diarrhea migrain sneez nausea stool congest depress throat cold fatigu mucus sore influenza fluenza cough medicin abdomin\n"
     ]
    }
   ],
   "source": [
    "keywords = {\"sneezing\", \"nasal\", \"congestion\", \"flu\", \"shot\", \"influenza\", \"fluenza\",\n",
    "            \"fever\", \"headache\", \"cough\", \"fatigue\", \"throat\",\n",
    "            \"sore\", \"mucus\", \"vaccine\", \"cold\", \"migraine\",\n",
    "            \"stool\", \"cramps\", \"abdominal\", \"pain\", \"blood\", \"nausea\", \"diarrhea\",\n",
    "            \"bloating\", \"injury\", \"accident\", \"stress\", \"surgery\", \"depression\",\n",
    "            \"bleeding\", \"medicine\", \"hospital\"}\n",
    "stem_keywords = [snowBallStemmer.stem(word) for word in keywords]\n",
    "print(' '.join(stem_keywords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actual keywords\n",
    "flu_only = {\"sneezing\", \"nasal\", \"congestion\", \"flu\", \"shot\", \"influenza\", \"fluenza\"}\n",
    "flu_general = {\"fever\", \"headache\", \"cough\", \"fatigue\", \"throat\",\n",
    "               \"sore\", \"mucus\", \"vaccine\", \"cold\", \"migraine\"}\n",
    "general = {\"stool\", \"cramps\", \"abdominal\", \"pain\", \"blood\", \"nausea\", \"diarrhea\",\n",
    "           \"bloating\", \"injury\", \"accident\", \"stress\", \"surgery\", \"depression\", \"bleeding\", \"medicine\", \"hospital\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "stem_flu_only = [snowBallStemmer.stem(word) for word in flu_only]\n",
    "stem_flu_general = [snowBallStemmer.stem(word) for word in flu_general]\n",
    "stem_general = [snowBallStemmer.stem(word) for word in general]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['influenza', 'fluenza', 'flu', 'sneez', 'congest', 'nasal', 'shot'],\n",
       " ['mucus',\n",
       "  'fever',\n",
       "  'sore',\n",
       "  'cough',\n",
       "  'migrain',\n",
       "  'headach',\n",
       "  'vaccin',\n",
       "  'throat',\n",
       "  'cold',\n",
       "  'fatigu'],\n",
       " ['pain',\n",
       "  'blood',\n",
       "  'hospit',\n",
       "  'depress',\n",
       "  'stress',\n",
       "  'accid',\n",
       "  'bleed',\n",
       "  'surgeri',\n",
       "  'diarrhea',\n",
       "  'medicin',\n",
       "  'cramp',\n",
       "  'nausea',\n",
       "  'stool',\n",
       "  'injuri',\n",
       "  'abdomin',\n",
       "  'bloat'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stem_flu_only, stem_flu_general, stem_general"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edits1(word):\n",
    "    \"All edits that are one edit away from `word`.\"\n",
    "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "def edits2(word): \n",
    "    \"All edits that are two edits away from `word`.\"\n",
    "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "flu_only_list = [set().union(edits1(i)) for i in stem_flu_only]\n",
    "flu_only_set =  set().union(*flu_only_list)\n",
    "# flu_only_set\n",
    "pickle.dump(flu_only_set, open(\"../data/nlp_fluonlywords.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "flu_general_list = [set().union(edits1(i)) for i in stem_flu_general]\n",
    "flu_general_set =  set().union(*flu_general_list)\n",
    "# flu_general_set\n",
    "pickle.dump(flu_general_set, open(\"../data/nlp_flugeneralwords.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_list = [set().union(edits1(i)) for i in stem_general]\n",
    "general_set =  set().union(*general_list)\n",
    "# general_set\n",
    "pickle.dump(general_set, open(\"../data/nlp_generalwords.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use nltk tokenizer. \n",
    "\n",
    "def flu_related_new(text):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    word_count = 0\n",
    "    for w in words:\n",
    "        if w in flu_general_set:\n",
    "            word_count+=1\n",
    "        elif w in flu_only_set:\n",
    "            word_count+=1.5\n",
    "    return word_count\n",
    "\n",
    "def general_disease_new(text):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    word_count = 0\n",
    "    for w in words:\n",
    "        if w in flu_general_set:\n",
    "            word_count+=1\n",
    "        elif w in general_set:\n",
    "            word_count+=1.5\n",
    "    return word_count\n",
    "\n",
    "# gen/flu>0.45\n",
    "# gen/flu<0.05\n",
    "\n",
    "\n",
    "def nlp_classify(text):\n",
    "    gen, flu = general_disease_new(text), flu_related_new(text)\n",
    "    if gen == 0 and flu ==0:\n",
    "        return \"neither\"\n",
    "    elif flu >= gen:\n",
    "        return \"flu\"\n",
    "    else:\n",
    "        return \"general\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'flu'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_classify(\"I have flu shot next to my apartment where the hospital just crashed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'neither'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_classify(\"I am in pcikle, please help me\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
