{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flu, Drug/Medicine, Mental Health Classifier\n",
    "\n",
    "The same NLP model for flu shots. But with better labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "snowBallStemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Actual keywords\n",
    "flu_only = {\"sneezing\", \"nasal\", \"congestion\", \"flu\", \"shot\", \"influenza\", \"fluenza\"}\n",
    "flu_general = {\"fever\", \"headache\", \"cough\", \"fatigue\", \"throat\",\n",
    "               \"sore\", \"mucus\", \"vaccine\", \"cold\", \"migraine\"}\n",
    "general = {\"stool\", \"cramps\", \"abdominal\", \"pain\", \"blood\", \"nausea\", \"diarrhea\",\n",
    "           \"bloating\", \"injury\", \"accident\", \"stress\", \"surgery\", \"depression\", \"bleeding\", \"medicine\", \"hospital\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New 3 class keywords\n",
    "flu_only = {\"flu\", \"shot\", \"influenza\", \"nasal\", \"fluenza\", \"congestion\"}\n",
    "\n",
    "flu_drug = {\"fever\", \"headache\", \"cough\", \"sneezing\", \"hospital\", \"sore\", \"mucus\", \"vaccine\", \"cold\", \"migraine\", \"runny-nose\"}\n",
    "\n",
    "mental_only = {\"depression\", \"depressed\", \"stress\", \"helpless\", \"hopeless\", \"anxious\", \"kms\", \"die\", \"cry\"}\n",
    "     \n",
    "mental_drug = {\"feel\", \"blood\", \"kill\", \"hallucinate\", \"hurts\"}\n",
    "\n",
    "drug_only = {\"throat\", \"vomit\", \"puke\", \"diarrhea\", \"stool\", \"cramps\", \"abdominal\", \"stomache\", \"bleeding\", \"nausea\", \"bloating\", \"injury\"}\n",
    "\n",
    "all_three = {\"fatigue\", \"pain\", \"sick\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the stems of the keywords.\n",
    "stem_flu_only = [snowBallStemmer.stem(word) for word in flu_only]\n",
    "stem_flu_drug = [snowBallStemmer.stem(word) for word in flu_drug]\n",
    "stem_mental_only = [snowBallStemmer.stem(word) for word in mental_only]\n",
    "stem_mental_drug = [snowBallStemmer.stem(word) for word in mental_drug]\n",
    "stem_drug_only = [snowBallStemmer.stem(word) for word in drug_only]\n",
    "stem_all_three = [snowBallStemmer.stem(word) for word in all_three]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consider typos in the text message.\n",
    "def edits1(word):\n",
    "    \"All edits that are one edit away from `word`.\"\n",
    "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "def edits2(word): \n",
    "    \"All edits that are two edits away from `word`.\"\n",
    "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "flu_only_list = [set().union(edits1(i)) for i in stem_flu_only]\n",
    "flu_only_set =  set().union(*flu_only_list)\n",
    "flu_only_set.update(flu_only)\n",
    "pickle.dump(flu_only_set, open(\"../data/nlp3_fluonlywords.pkl\", \"wb\"))\n",
    "\n",
    "flu_drug_list = [set().union(edits1(i)) for i in stem_flu_drug]\n",
    "flu_drug_set =  set().union(*flu_drug_list)\n",
    "flu_drug_set.update(flu_drug)\n",
    "pickle.dump(flu_drug_set, open(\"../data/nlp3_fludrugwords.pkl\", \"wb\"))\n",
    "\n",
    "mental_only_list = [set().union(edits1(i)) for i in stem_mental_only]\n",
    "mental_only_set =  set().union(*mental_only_list)\n",
    "mental_only_set.update(mental_only)\n",
    "pickle.dump(mental_only_set, open(\"../data/nlp3_mentalonlywords.pkl\", \"wb\"))\n",
    "\n",
    "mental_drug_list = [set().union(edits1(i)) for i in stem_mental_drug]\n",
    "mental_drug_set =  set().union(*mental_drug_list)\n",
    "mental_drug_set.update(mental_drug)\n",
    "pickle.dump(mental_drug_set, open(\"../data/nlp3_mentaldrugwords.pkl\", \"wb\"))\n",
    "\n",
    "drug_only_list = [set().union(edits1(i)) for i in stem_drug_only]\n",
    "drug_only_set =  set().union(*drug_only_list)\n",
    "drug_only_set.update(drug_only)\n",
    "pickle.dump(drug_only_set, open(\"../data/nlp3_drugonlywords.pkl\", \"wb\"))\n",
    "\n",
    "all_three_list = [set().union(edits1(i)) for i in stem_all_three]\n",
    "all_three_set =  set().union(*all_three_list)\n",
    "all_three_set.update(all_three)\n",
    "pickle.dump(all_three_set, open(\"../data/nlp3_allthreewords.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"killing\" in mental_only_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flu_weight(text):\n",
    "    wordList = nltk.word_tokenize(text)\n",
    "    words = [snowBallStemmer.stem(word) for word in wordList]\n",
    "    word_weight = 0\n",
    "    for w in words:\n",
    "        if w in flu_drug_set:\n",
    "            word_weight+=1.5\n",
    "        elif w in flu_only_set:\n",
    "            word_weight+=2\n",
    "        elif w in all_three_set:\n",
    "            word_weight+=1\n",
    "    return word_weight\n",
    "\n",
    "def drug_weight(text):\n",
    "    wordList = nltk.word_tokenize(text)\n",
    "    words = [snowBallStemmer.stem(word) for word in wordList]\n",
    "    word_weight = 0\n",
    "    for w in words:\n",
    "        if w in flu_drug_set:\n",
    "            word_weight+=1.5\n",
    "        elif w in drug_only_set or w in mental_drug_set:\n",
    "            word_weight+=2\n",
    "        elif w in all_three_set:\n",
    "            word_weight+=1\n",
    "    return word_weight\n",
    "\n",
    "def mental_weight(text):\n",
    "    wordList = nltk.word_tokenize(text)\n",
    "    words = [snowBallStemmer.stem(word) for word in wordList]\n",
    "    word_weight = 0\n",
    "    for w in words:\n",
    "        print(w)\n",
    "        if w in mental_only_set:\n",
    "            word_weight+=2\n",
    "        elif w in mental_drug_set:\n",
    "            word_weight+=1.5\n",
    "        elif w in all_three_set:\n",
    "            word_weight+=1\n",
    "    return word_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i\n",
      "have\n",
      "a\n",
      "runni\n",
      "nose\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1.5, 1.5, 0)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flu_weight(\"I have a runny nose\"), drug_weight(\"I have a runny nose\"), mental_weight(\"I have a runny nose\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the\n",
      "pain\n",
      "is\n",
      "kill\n",
      "me\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0, 3, 2.5)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentence = \"The pain is killing me\"\n",
    "flu_weight(\"I feel hurts\"), drug_weight(test_sentence), mental_weight(test_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nlp_classify(text):\n",
    "    flu, drug, mental = flu_weight(text), drug_weight(text), mental_weight(text) \n",
    "    max_weight = max(flu, drug, mental)\n",
    "    if drug == 0 and flu ==0 and mental==0:\n",
    "        return \"neither\"\n",
    "    elif drug == max_weight:\n",
    "        return \"drug\"\n",
    "    elif flu == max_weight:\n",
    "        return \"flu\"\n",
    "    else:\n",
    "        return \"mental\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my\n",
      "pain\n",
      "is\n",
      "kill\n",
      "me\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'drug'"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_classify(\"My pain is killing me\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test for classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/nlp3_fluonlywords.pkl\", 'rb') as pickle_file:\n",
    "    flu_only_set = pickle.load(pickle_file)\n",
    "with open(\"../data/nlp3_mentalonlywords.pkl\", 'rb') as pickle_file:\n",
    "    mental_only_set = pickle.load(pickle_file)\n",
    "with open(\"../data/nlp3_mentaldrugwords.pkl\", 'rb') as pickle_file:\n",
    "    mental_drug_set = pickle.load(pickle_file)\n",
    "with open(\"../data/nlp3_fludrugwords.pkl\", 'rb') as pickle_file:\n",
    "    flu_drug_set = pickle.load(pickle_file)\n",
    "with open(\"../data/nlp3_drugonlywords.pkl\", 'rb') as pickle_file:\n",
    "    drug_only_set = pickle.load(pickle_file)\n",
    "with open(\"../data/nlp3_allthreewords.pkl\", 'rb') as pickle_file:\n",
    "    all_three_set = pickle.load(pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "none\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'neither'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_classify(\"none\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
